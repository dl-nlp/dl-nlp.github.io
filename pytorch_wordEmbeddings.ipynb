{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP WS19/20\n",
    "## Exercise Sheet 5 - Pytorch WordEmbeddings\n",
    "This exercise sheet is due on 26.11.19 11:59 pm. There is a total of 5\n",
    "points for this exercise sheet. Please send your solution in a\n",
    "suitable format to [profilmodul1920@cis.lmu.de](mailto:profilmodul1920@cis.lmu.de). Please submit a\n",
    "completed version of this file in Python 3 and please submit in teams of\n",
    "2 or 3 students.\n",
    "\n",
    "\n",
    "You will have to complete the code/questions marked with ***TODO***\n",
    "\n",
    "Please rename the file to pytorch_wordEmbeddings_last_names.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please refer to the last exercise sheet if you have trouble installing Pytorch. This time you will have to install nltk. (e.g. by using pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk library provides datasets which can be used for dry running an approach or to verify a hypothesis. We will use the brown corpus. If you don't yet have the brown corpus on your machine, run the following cell once to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"brown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For developing/debugging your code you may want to use only the first tokens of the corpus. Training with the full corpus takes quite some time. Use only the first 1000 words until your code works, later you can use the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_word_list = [w.lower() for w in brown.words()] #TODO only use a subset during developing/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the vocabulary. The brown corpus contains 56057 unique words/tokens (lower cased), for the sake of computation time we will only use the 10 000 most common words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary\n",
    "word_counts = Counter(brown_word_list)\n",
    "vocab_size = 10000  # \n",
    "vocab = {w[0]: idx for idx, w in enumerate(word_counts.most_common(vocab_size))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some Hyper-parameters. Make sure you understand what each of these do. You can always send questions to [profilmodul1819@cis.lmu.de](mailto:profilmodul1819@cis.lmu.de)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of the algorithm\n",
    "window_size = 1 # window size\n",
    "# You can change the number of negative samples to a higher value, e.g. 7. \n",
    "# This will give you better results, but will take longer to train.\n",
    "neg_samples_factor = 2 # negative samples multiple\n",
    "dims = 64 # embedding dimension\n",
    "learning_rate = 0.01\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a list of strings (words) and returns a generator of word tuples of the following form:\n",
    "    \n",
    "(center_word, context_word, label)\n",
    "\n",
    "where center_word is the word at a certain position, and context_word is at most max_distance tokens away.\n",
    "\n",
    "Each pair is marked by label (positive cooccurrences have the label \"True\", negative cooccurrences are marked with \"False\").\n",
    "\n",
    "Words are represented by integers (rather than by string) denoting their id.\n",
    "\n",
    "Only pairs where both words are in the vocabulary are considered.\n",
    "\n",
    "Note: cooccurrence only holds between words in different positions, not for a position with itself.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_and_negative_cooccurrences(tokens, max_distance, neg_samples_factor, vocab_to_id):\n",
    "    \"\"\"\n",
    "    :param tokens: list of strings (words)\n",
    "    :param max_distance: max distance of context word to target word\n",
    "    :param neg_samples_factor: number of sampled negative tuples for each positive tuple\n",
    "    :param vocab_to_id: dictionary (string to int) mapping each word to its id (=row in embedding matrizes)\n",
    "    :return: generator over tuples of the form (context_word:string, center_word:string, label:boolean)\n",
    "    \"\"\"\n",
    "    for center_position in range(len(tokens)):\n",
    "        center_word = tokens[center_position]\n",
    "        if center_word not in vocab_to_id:\n",
    "            continue\n",
    "        context_start = max(0, center_position - max_distance)\n",
    "        context_end = min(len(tokens), center_position + max_distance + 1)\n",
    "        for context_position in range(context_start, context_end):\n",
    "            if context_position != center_position:\n",
    "                context_word = tokens[context_position]\n",
    "                if context_word not in vocab_to_id:\n",
    "                    continue\n",
    "                yield (vocab_to_id[center_word], vocab_to_id[context_word], True)\n",
    "                for i in range(neg_samples_factor):\n",
    "                    yield (vocab_to_id[center_word], random.randint(0, len(vocab_to_id) - 1), False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build our model. As you have seen in the last exercise you define a model by making a class inherit from torch.nn.module as well as defining the init and forward method. \n",
    "\n",
    "##### init method\n",
    "\n",
    "***TODO*** You will have to create two embedddings for the vocabulary as center and context words. Hint: There is an Embedding layer in Pytorch which should be used here. (0.5p)\n",
    "\n",
    "***TODO*** How are the weight values in the embedding layer initialized internally? (0.5p) \n",
    "\n",
    "***TODO*** Inspect the stucture of the embedding layer and change the initial weights to a torch Tensor of normally distributed float values with 0 mean and variance 0.1. (Consulting the documentation for torch.Tensor might help.) (1p)\n",
    "\n",
    "***TODO*** Can you think of a scenario, in which you would initialize the embedding weights with specific values? (0.5p)\n",
    "\n",
    "#### forward method:\n",
    "\n",
    "***TODO*** What is the shape of center_context_idxs? (0.5p)\n",
    "\n",
    "***TODO*** What are the shapes of cntr_idxs and ctxt_idxs? (0.5p)\n",
    "\n",
    "***TODO*** What shape does ctxt_vecs have? What shape does torch.bmm require ctxt_vecs to have? (0.5p)\n",
    "\n",
    "***TODO*** Bring ctxt_vecs into the required shape (use variable.view(...) ) (0.5p)\n",
    "\n",
    "***TODO*** What is the shape of the returned Tensor? (0.5p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        # TODO: create embeddings for vocabulary as center and context words. (0.5p)\n",
    "        self.embeddings_center = None # TODO\n",
    "        self.embeddings_context = None #TODO\n",
    "        # TODO: Initialize the word vectors so that the components are normally distributed \n",
    "        # and have mean 0 and variance 0.1 (1p)\n",
    "        \n",
    "    def forward(self, center_context_idxs):\n",
    "        # TODO: What is the shape of center_context_idxs? (0.5p)\n",
    "        cntr_idxs = center_context_idxs[:, 0]\n",
    "        ctxt_idxs = center_context_idxs[:, 1]\n",
    "        # TODO: What are the shapes of cntr_idxs and ctxt_idxs? (0.5p)\n",
    "        \n",
    "        # Bring the center embeddings into the required shape for torch.bnn()\n",
    "        cntr_vecs = self.embeddings_center(ctxt_idxs).view(-1, 1, self.embedding_size)\n",
    "        # resulting shape: batch_size x 1 x embedding_size\n",
    "        \n",
    "        ctxt_vecs = self.embeddings_context(cntr_idxs) #TODO\n",
    "        # TODO: What shape does ctxt_vecs have? What shape does torch.bmm() require ctxt_vecs to have? (0.5p)\n",
    "        # TODO: Bring ctxt_vecs into the required shape (using variable.view(...) ) (0.5p)\n",
    "       \n",
    "        scores = torch.bmm(cntr_vecs, ctxt_vecs) # Batch-wise matrix multiplication.\n",
    "        # resulting shape: batch_size x 1 x 1\n",
    "        \n",
    "        # TODO: What is the shape of the returned Tensor? (0.5p)\n",
    "        return scores.view(-1,1) \n",
    "\n",
    "    def center_sims(self, word_idx):\n",
    "        m = self.embeddings_center.weight\n",
    "        v = m[word_idx]\n",
    "        return F.cosine_similarity(m, v.expand(m.size())).data.numpy()\n",
    "\n",
    "    def context_sims(self, word_idx):\n",
    "        m = self.embeddings_context.weight\n",
    "        v = m[word_idx]\n",
    "        return F.cosine_similarity(m, v.expand(m.size())).data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on there is nothing left to do for you. Please read through the following and have a look into the documentation to see what each line does and feel free to experiment with different hyperparameters.\n",
    "\n",
    "Understanding how the model learns and predicts will be essential for your upcoming projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(vocab_size, dims)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(w2v_model.parameters(), lr=learning_rate)\n",
    "\n",
    "pos_neg_list = list(positive_and_negative_cooccurrences(brown_word_list, window_size, neg_samples_factor, vocab))\n",
    "data_size=len(pos_neg_list)\n",
    "train_data = np.asarray(pos_neg_list)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "data_tensor = torch.LongTensor(train_data[:, 0:2].tolist())\n",
    "target_tensor = torch.FloatTensor(train_data[:, 2].tolist()).view(-1,1)\n",
    "\n",
    "train = data_utils.TensorDataset(data_tensor, target_tensor)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch_nr in range(num_epochs):\n",
    "    loss_accum = 0.0\n",
    "    print(\"epoch\", epoch_nr)\n",
    "    for ctxt_tgt_idxs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = w2v_model.forward(ctxt_tgt_idxs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss_accum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"current loss:\", loss_accum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we get the most similar word embeddings to a few example words. The first list holds the results from the word being the center word, the second from the word being a context word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words = sorted(vocab.keys(), key=vocab.get)\n",
    "def top_words_for(word, n=10, for_center=True):\n",
    "    print(word)\n",
    "    query_index = vocab[word]\n",
    "    if for_center:\n",
    "        sims = list(zip(w2v_model.center_sims(query_index), sorted_words))\n",
    "    else:\n",
    "        sims = list(zip(w2v_model.context_sims(query_index), sorted_words))\n",
    "    sims.sort(key=lambda x: -x[0])\n",
    "    return sims[:n]\n",
    "\n",
    "print(\"word\".ljust(20), \"similarity\")\n",
    "for ww in [\"the\", \"jury\", \"city\", \"man\", \"any\",\"1\",\"two\"]:\n",
    "    print(\"=\"*35)\n",
    "    for dist, nw in top_words_for(ww, for_center=True):\n",
    "        print(nw.ljust(20), dist)\n",
    "    for dist, nw in top_words_for(ww, for_center=False):\n",
    "        print(nw.ljust(20), dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
