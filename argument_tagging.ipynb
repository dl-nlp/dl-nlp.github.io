{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spoken Language Understanding / Argument Tagging\n",
    "\n",
    "This homework is an adaptation of this Theano tutorial: http://deeplearning.net/tutorial/rnnslu.html\n",
    "\n",
    "In this homework, you will train a Keras model for the Spoken Language Understanding task, which consists in assigning a label to each word given a sentence. Itâ€™s a sequence labelling task.\n",
    "\n",
    "An old and small benchmark for this task is the ATIS (Airline Travel Information System) dataset collected by DARPA. Here is a sentence (or utterance) example using the Inside Outside Beginning (IOB) representation.\n",
    "\n",
    "\n",
    "|Input (words)|show|flights|from|Boston|to|New|York|today|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|Output (labels)|O|O|O|B-dept|O|B-arr|I-arr|B-date|\n",
    "\n",
    "The ATIS offical split contains 4,978/893 sentences for a total of 56,590/9,198 words (average sentence length is 15) in the train/test set. The number of classes (different slots) is 128 including the O label (NULL).\n",
    "\n",
    "Unseen words in the test are dealt with set by marking any words with only one single occurrence in the training set as <UNK> and use this token to represent those unseen words in the test set. Sequences of numbers are converted to repetitions of the string DIGIT i.e. 1984 is converted to DIGITDIGITDIGITDIGIT.\n",
    "    \n",
    "** There are 10 points in total for this homework. Send the completed notebook to beroth@cis.uni-muenchen.de. The deadline is Tuesday, December 12, 23:59. You can work in teams of 2 or 3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to download the data from the course homepage (atis.json). Then you can load it into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from keras.utils import to_categorical\n",
    "import json\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
    "np.random.seed(1)\n",
    "\n",
    "with open(\"atis.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_dev_sents = data[\"train_sents\"] # list of lists\n",
    "train_dev_labels = data[\"train_labels\"] # list of lists\n",
    "num_train = math.floor(0.8 * len(train_dev_sents))\n",
    "train_sents = train_dev_sents[:num_train]\n",
    "train_labels = train_dev_labels[:num_train]\n",
    "dev_sents = train_dev_sents[num_train:]\n",
    "dev_labels = train_dev_labels[num_train:]\n",
    "test_sents = data[\"test_sents\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "word_to_id = data[\"vocab\"]\n",
    "label_to_id = data[\"label_dict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: How many sentences are in the train, dev and test set? (0.5 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define some constants that we'll use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "VOCAB_SIZE = len(word_to_id)\n",
    "NUM_LABELS = len(label_to_id)\n",
    "EMBEDDING_SIZE = 50\n",
    "HIDDEN_SIZE=50\n",
    "MAX_LENGTH=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: What are the token ids of the \"&lt;UNK>\" and \"&lt;PAD>\" token, and what is the label id of \"O\"? (0.5 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: print the string representation of the words (not ids) in the first sentence. (1 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here.\n",
    "# You may find it helpful to create dictionaries from ids to strings:\n",
    "id_to_word = dict() # TODO\n",
    "id_to_label = dict() # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's bring the data into the format needed by Keras\n",
    "\n",
    "**TODO: create numpy matrix of size: num_training_sentences x MAX_LENGTH. Do the same for dev and test set. (2 p.)**\n",
    "\n",
    "** Hint: ** Use the Keras methods pad_sequences, to trim/expand exactly to the desired length: https://keras.io/preprocessing/sequence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_padding(sequences, length, padding_value):\n",
    "    pass # TODO\n",
    "train_sents_padded = None # do_padding(...) # TODO\n",
    "dev_sents_padded = None #do_padding(...) # TODO\n",
    "test_sents_padded = None #do_padding(...) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the labels.\n",
    "\n",
    "** TODO: Create numpy matrices to encode the labels. In addition to padding, you need to transform the label ids into \"one-hot encodings\", vectors that are 1 for the specified label, and 0 otherwise. The resulting matrices should have shape num_sentences x MAX_LENGTH x NUM_LABELS. (1 p.) **\n",
    "\n",
    "** Hint: **  Use the keras method to_categorical. https://keras.io/utils/#to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_padded = None # TODO\n",
    "dev_labels_padded = None # TODO\n",
    "test_labels_padded = None # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to define the LSTM model. It consists of the following components:\n",
    "* An embedding layer that learns word vectors that are passed on to the lstm as an input.\n",
    "* The LSTM layer. We use a bidirectional LSTM to consider information from left and right.\n",
    "* A final layer that predicts a label for each position in the sentence from the LSTM hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: Create the embedding layer for the vocabulary. It learns lookup vectors (size: EMBEDDING_SIZE) for all words in the vocabulary. We store the embedding layer in an extra variable so that we can inspect it later.  (1 p.)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = None # TODO\n",
    "model.add(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add a bidirectional lstm layer with hidden units of size HIDDEN_SIZE. The lstm should return the sequence of hidden states. (1 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: Output a prediction over the possible labels (i.e. use the softmax activation) at each time step, i.e. apply a Dense layer with softmax activation at each time step. (1 p.)**\n",
    "\n",
    "** Hint: ** Use TimeDistributed: https://keras.io/layers/wrappers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model with the 'adam' optimizer and 'categorical_crossentropy' as the loss (this corresponds to negative log-likelihood). We also monitor the 'acurracy' as the metric of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_sents_padded, train_labels_padded, batch_size=8, epochs=10, \\\n",
    "          validation_data=(dev_sents_padded, dev_labels_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: Now, predict the label sequence for the first sentence in the dev data. Print both the sentence and the predicted labes (words/labels, not ids.) (1 p.)**\n",
    "\n",
    "** Hint:** You can use model.predict_classes(...) to obtain the predicted label ids. Which shape does predict_classes(...) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not only good for labelling sequences, it also learns word vectors. Let's inspect them.\n",
    "\n",
    "** TODO: For each of the query_words, print the 10 words with the highest cosine similarity. (1 p.) **\n",
    "\n",
    "**Hint:** You can obtain the learned embedding matrix using:\n",
    "`learned_embeddings = embedding_layer.get_weights()[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities(m, v):\n",
    "    \"\"\" Computes cosine for each row in a numpy matrix m with a vector v.\"\"\"\n",
    "    norm_m = np.sqrt((m**2).sum(1))\n",
    "    norm_v = np.sqrt(v.dot(v))\n",
    "    return m.dot(v) / (norm_m * norm_v)\n",
    "\n",
    "query_words = [\"boston\", \"saturday\", \"DIGIT\", \"today\", \"nonstop\", \"am\"]\n",
    "\n",
    "# TODO: Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
