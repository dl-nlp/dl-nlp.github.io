{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spoken Language Understanding / Argument Tagging\n",
    "\n",
    "**There are 10 points in total for this homework. Send the completed notebook to [anne.beyer@campus.lmu.de](mailto:anne.beyer@campus.lmu.de). The deadline is Tuesday, December 4, 23:59. Please submit a completed version of this file in Python 3 in teams of 2 or 3 students.**\n",
    "\n",
    "**Please rename the file to argument_tagging_last_names.ipynb**\n",
    "\n",
    "This homework is an adaptation of this Theano tutorial: http://deeplearning.net/tutorial/rnnslu.html\n",
    "\n",
    "In this homework, you will train a Keras model for the Spoken Language Understanding task, which consists in assigning a label to each word given a sentence. Itâ€™s a sequence labelling task.\n",
    "\n",
    "An old and small benchmark for this task is the ATIS (Airline Travel Information System) dataset collected by DARPA. Here is a sentence (or utterance) example using the Inside Outside Beginning (IOB) representation.\n",
    "\n",
    "\n",
    "|Input (words)|show|flights|from|Boston|to|New|York|today|\n",
    "\n",
    "|Output (labels)|O|O|O|B-dept|O|B-arr|I-arr|B-date|\n",
    "\n",
    "The ATIS offical split contains 4,978/893 sentences for a total of 56,590/9,198 words (average sentence length is 15) in the train/test set. The number of classes (different slots) is 128 including the O label (Outside).\n",
    "\n",
    "Unseen words in the test are dealt with set by marking any words with only one single occurrence in the training set as \"&lt;UNK>\" and to use this token to represent those unseen words in the test set. Sequences of numbers are converted to repetitions of the string DIGIT i.e. 1984 is converted to DIGITDIGITDIGITDIGIT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the classes and functions we will be needing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, GRU, TimeDistributed, Dense, Bidirectional, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to download the data from the course homepage (atis.json) into your current directory. Then you can load it into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"atis.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract data from file\n",
    "word_to_id = data[\"vocab\"]\n",
    "label_to_id = data[\"label_dict\"]\n",
    "test_sents = data[\"test_sents\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "\n",
    "# we also want to create a dev set by splitting the training data\n",
    "train_dev_sents = data[\"train_sents\"] # list of lists\n",
    "train_dev_labels = data[\"train_labels\"] # list of lists\n",
    "num_train = math.floor(0.8 * len(train_dev_sents))\n",
    "\n",
    "train_sents = train_dev_sents[:num_train]\n",
    "train_labels = train_dev_labels[:num_train]\n",
    "\n",
    "dev_sents = train_dev_sents[num_train:]\n",
    "dev_labels = train_dev_labels[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: How many sentences are in the train, dev and test set? (0.5 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define some constants that we'll use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "VOCAB_SIZE = len(word_to_id)\n",
    "NUM_LABELS = len(label_to_id)\n",
    "EMBEDDING_SIZE = 50\n",
    "HIDDEN_SIZE=50\n",
    "MAX_LENGTH=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: What is the token id of the \"&lt;PAD>\" token, and what is the label id of \"O\"? (0.5 p.)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Reorganize the data (train, dev, test) so that the token \"&lt;PAD>\" has id 0 and the label 'O' has id 0 (and all words are still represented). Make sure to also change word_to_id and label_to_id (1 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: print the string representation of the words (not ids) in the first test sentence. (0.5 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may find it helpful to create dictionaries from ids to strings:\n",
    "id_to_word = {} # TODO\n",
    "id_to_label = {} # TODO\n",
    "\n",
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's bring the data into the format needed by Keras\n",
    "\n",
    "**TODO: create input matrix of size: num_training_sentences x MAX_LENGTH. Do the same for dev and test set. (1 p.)**\n",
    "\n",
    "**Hint:** Use the Keras methods pad_sequences, to trim/expand exactly to the desired length: https://keras.io/preprocessing/sequence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_padding(sequences, length = MAX_LENGTH):\n",
    "    return None # TODO\n",
    "\n",
    "train_sents_padded = do_padding(train_sents)\n",
    "dev_sents_padded = do_padding(dev_sents) \n",
    "test_sents_padded = do_padding(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the labels.\n",
    "\n",
    "**TODO: Create numpy matrices to encode the labels. In addition to your do_padding function, you need to transform the label ids into \"one-hot encodings\" (vectors that are 1 for the specified label, and 0 otherwise). The resulting matrices should have shape num_sentences x MAX_LENGTH x NUM_LABELS. (1 p.)**\n",
    "\n",
    "**Hint:**  Use the keras function to_categorical. https://keras.io/utils/#to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_padded = None # TODO\n",
    "dev_labels_padded = None # TODO\n",
    "test_labels_padded = None # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to define the GRU model. It consists of the following components:\n",
    "* An embedding layer that learns word vectors that are passed on to the GRU as an input.\n",
    "* The GRU layer. We use a bidirectional GRU to consider information from left and right.\n",
    "* A final layer that predicts a label for each position in the sentence from the GRU hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Create the embedding layer for the vocabulary. It learns lookup vectors (size: EMBEDDING_SIZE) for all words in the vocabulary. Make sure to enable masking of the &lt;PAD> words! (1 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Apply 50% dropout to the embeddings by adding a Dropout layer (0.5 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Add a bidirectional GRU layer with hidden units of size HIDDEN_SIZE. The GRU should return the sequence of hidden states. Apply L2 regularization with a weight of 0.001 to the GRU kernel and bias. (1 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Output a prediction over the possible labels at each time step, i.e. apply a Dense layer with softmax activation at each time step. (1 p.)**\n",
    "\n",
    "**Hint:** Use the TimeDistributed wrapper: https://keras.io/layers/wrappers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model with the 'adam' optimizer and 'categorical_crossentropy' as the loss (this corresponds to negative log-likelihood). We also monitor the 'acurracy' as the metric of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: implement a callback that stops training when the development set loss (\"val_loss\") has not decreased for 2 epochs (0.5 p.)**\n",
    "\n",
    "**Hint:** https://keras.io/callbacks/#earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = None # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_sents_padded, train_labels_padded, batch_size=8, \\\n",
    "          callbacks = [earlystop], epochs=100, \\\n",
    "          validation_data=(dev_sents_padded, dev_labels_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Now, evaluate the model on the test data. (0.5 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Predict the label sequence for the first sentence in the test data. Print both the sentence and the predicted labels (words/label strings, not ids.) (1 p.)**\n",
    "\n",
    "**Hint:** You can use model.predict_classes(...) to obtain the predicted label ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** \n",
    "\n",
    "Feel free to experiment with different settings (without zero-masking/Dropout/Regularization, change Dropout rate/l2 weights/optimizer/metrics ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
