{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP WS18/19\n",
    "## Exercise Sheet 3 - Pytorch Introduction\n",
    "This exercise sheet is due on 19.11.19 11:59 pm. There is a total of 10\n",
    "points for this exercise sheet. Please send your solution to [profilmodul1920@cis.uni-muenchen.de](mailto:profilmodul1920@cis.uni-muenchen.de). Please submit a completed version of this file in Python 3. You may submit in teams of 2 or 3 students.\n",
    "\n",
    "Please rename the file to pytorch_intro_last_names.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of required packages\n",
    "\n",
    "For installation of Pytorch check <http://pytorch.org/>. Note that CUDA is required if you want to execute Pytorch on a GPU. The program below doesn't require a lot of computation, so CPU-only is enough.\n",
    "\n",
    "The sklearn can be installed with pip (pip3 for python3) or with the procedure you chose for the last exercise sheet and installing numpy.\n",
    "\n",
    "#### If you have any problems regarding the installation feel free to send an email to [profilmodul1819@cis.lmu.de](mailto:profilmodul1819@cis.lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "#### Linear Regression on the California Housing Dataset (5 points)\n",
    "\n",
    "Useful Pytorch tutorials and resources are mentioned in the lecture slides. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The California Housing Dataset is often used as a logistic regression example. The dataset is provided with the sklearn module.\n",
    "\n",
    "You will have to complete the code where marked with ***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import Pytorch, the Boston dataset, numpy, math and shuffle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import math\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a method which returns 3 boolean arrays that helps providing a shuffled train, dev, test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_train_dev_test_split(num_total_items, train_ratio = 0.5, dev_ratio = 0.25):\n",
    "    num_train_items = math.floor(num_total_items * train_ratio)\n",
    "    num_dev_items = math.floor(num_total_items * dev_ratio)\n",
    "    num_test_items = num_total_items - num_train_items - num_dev_items\n",
    "    split = [0] * num_train_items + [1] * num_dev_items + [2] * num_test_items\n",
    "    shuffle(split)\n",
    "    split = np.asarray(split)\n",
    "    return split == 0, split == 1, split == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the main program. At first we get the dataset from the sklearn module and assign features and labels to x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = sklearn.datasets.fetch_california_housing()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "num_items, num_features = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: normalize the data so that each feature has 0 mean and unit standard deviation (1 p.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get a 50%-25%-25% split of the shuffled(!) training data (you can use the provided method random_train_dev_test_split).\n",
    "\n",
    "If you use the predefined method, note:\n",
    " * The returned arrays contain boolean indicators which element are contained in the respective sets\n",
    " * You can use Boolean indexing and Numpy to access those elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spl, dev_spl, test_spl = random_train_dev_test_split(num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: get a k x n -dimensional numpy array, where k is the number of items in the training data, n the number of features. (0.5 p.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: get a k x 1 dimensional numpy array of the training targets. Hint: use np.expand_dims(...) to get an extra dimension (i.e. a matrix instead of an vector. Check the shape of train_y to see the difference). (0.5 p.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Similarly, get the dev feature matrix with associated dev targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_x = # TODO\n",
    "dev_y = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the Linear Regression class that inherits from the Pytorch nn module. Information about __init__ and __forward__ can be found in the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.final_layer = nn.Linear(num_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.final_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Mean Squared Error as loss criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __np_to_torch__ method is a convinience method used later to convert numpy Arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(np_array):\n",
    "    return torch.from_numpy(np_array).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training iterates over the data 20 times. The gradient is backpropagated after every training example and total loss is printed at the end of each epoch.\n",
    "#### TODO: Extend the code that it prints the average per-example loss on development data at the end of each epoch. (1 p.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "linreg_model = LinearRegression(num_features)\n",
    "optimizer = optim.SGD(linreg_model.parameters(), lr=0.0001)\n",
    "\n",
    "#Train model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    loss_accum = 0.0\n",
    "    for i in range(len(train_y)):\n",
    "        x_i = np_to_torch(train_x[i])\n",
    "        y_i = np_to_torch(train_y[i])\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = linreg_model.forward(x_i)\n",
    "        loss = criterion(output, y_i)\n",
    "        loss_accum += loss.data.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    # Evaluate model\n",
    "    print(\"train loss:\", loss_accum/len(train_y))\n",
    "    # TODO: Also print the dev loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some gradient-based optimizers work well with looking at the entire data set for one update step. One of those optimizers is LBFGS.\n",
    "#### TODO: replace the SGD optimizer by using the LBFGS optimizer. Check the Pytorch documentation for the optim package for more information: <http://pytorch.org/docs/master/optim.html> (2 p.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model\n",
    "linreg_model = LinearRegression(num_features)\n",
    "optimizer = #TODO: LBFGS instead of SGD optimizer.\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = linreg_model.forward(np_to_torch(train_x))\n",
    "    loss = criterion(output, np_to_torch(train_y))\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain new model\n",
    "# TODO: perform the optimizer step (once).\n",
    "\n",
    "# Evaluate new model\n",
    "# TODO: Print train and dev loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "#### Neural Network Regression on the California Housing Dataset (5 points)\n",
    "Now implement the regression with a hidden layer and an activation function after that hidden layer. \n",
    "\n",
    "Recall that normal linear regression uses the function:\n",
    "\n",
    "$$\\hat y = Linear(x) = Wx+b$$\n",
    "\n",
    "Neural Network regression uses two (distinct) linear transformations, and a non-linear activation function in between. Using the tanh activation function, we get:\n",
    "\n",
    "$$\\hat y = Linear(Tanh(Linear(x))) = W_B tanh(W_Ax + b_A) + b_B$$\n",
    "\n",
    "The output of the first linear transformation (+ non-linear activation) is also called the hidden layer.\n",
    "You can use for example 10 as its size (hidden_size).\n",
    "\n",
    "#### TODO: You need to change the __init__ and the __forward__ method. (2 p.)\n",
    "\n",
    "See <https://pytorch.org/docs/stable/nn.html?#non-linear-activations-weighted-sum-nonlinearity> for a list of pre-defined activation functions\n",
    "\n",
    "Optional: Feel free to experiment with different activation functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN model\n",
    "class NeuralNetworkRegression(nn.Module):\n",
    "    def __init__(self, num_features,hidden_size):\n",
    "        super(NeuralNetworkRegression, self).__init__()\n",
    "        self.final_layer = nn.Linear(num_features, 1) # TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.final_layer((x)) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Analogous to the Linear Regression model, create a Neural Regression model, train it using the LBFGS optimizer and report the MSE on the train and dev set. (2 p.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN model\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NN model\n",
    "# TODO\n",
    "\n",
    "# Evaluate NN model\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: How does the Neural Network Regression compare to Linear Regression? (1 p.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Feel free to experiment with different optimizers for both models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
