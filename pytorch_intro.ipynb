{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP WS17/18\n",
    "## Exercise Sheet 3 - Pytorch Introduction\n",
    "This exercise sheet is due on 21.11.17 11:59 pm. There is a total of 12\n",
    "points for this exercise sheet. Please send your solution in a\n",
    "suitable format to [beroth@cis.uni-muenchen.de](mailto:beroth@cis.uni-muenchen.de). Please submit a\n",
    "completed version of this file in Python 3. You may submit in teams of\n",
    "2 or 3 students.\n",
    "\n",
    "Please rename the file to pytorch_intro_last_names.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of required packages\n",
    "\n",
    "For installation of Pytorch check <http://pytorch.org/>. You need to select the specifc __wheel__ to make it work. Note that CUDA is required if you want to execute Pytorch on a GPU. The program below doesn't require a lot of computation, so CPU-only is enough.\n",
    "\n",
    "The sklearn can be installed with pip (pip3 for python3) or with the procedure you chose for the last exercise sheet and installing numpy.\n",
    "\n",
    "#### If you have any problems regarding the installation feel free to send an email to [simon.h.schaefer@googlemail.com](mailto:simon.h.schaefer@googlemail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "#### Logistic regression on the Boston Housing Dataset (9 points)\n",
    "\n",
    "Usefull Pytorch tutorials and resources are mentioned in the lecture slides. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston Housing Dataset is often used as a logistic regression example. It has 14 features and 2 prototasks. See <https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html> for details. The dataset is provided with the sklearn module.\n",
    "\n",
    "You will have to complete the code where marked with ***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import Pytorch, the Boston dataset, math and shuffle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.datasets\n",
    "from random import shuffle\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a method which returns 3 boolean arrays that helps providing a shuffled train, dev, test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_train_dev_test_split(num_total_items, train_ratio = 0.5, dev_ratio = 0.25):\n",
    "    num_train_items = math.floor(num_total_items * train_ratio)\n",
    "    num_dev_items = math.floor(num_total_items * dev_ratio)\n",
    "    num_test_items = num_total_items - num_train_items - num_dev_items\n",
    "    split = [0] * num_train_items + [1] * num_dev_items + [2] * num_test_items\n",
    "    shuffle(split)\n",
    "    split = np.asarray(split)\n",
    "    return split == 0, split == 1, split == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the main program. At first we get the dataset from the sklearn module and assign features and labels to x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = sklearn.datasets.load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "num_items, num_features = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a 50%-25%-25% split of the shuffled(!) training data (you can use the provided method random_train_dev_test_split).\n",
    "If you use the predefined method, note:\n",
    " * The returned arrays contain boolean indicators which element are contained in the respective sets\n",
    " * You can use Boolean indexing and Numpy to access those elements\n",
    "#### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_spl, dev_spl, test_spl = random_train_dev_test_split(num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: get a k x n -dimensional numpy array, where k is the number of items in the training data, n the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: get a k x 1 dimensional numpy array of the training targets. Hint: use np.expand_dims(...) to get an extra dimension (i.e. a matrix instead of an vector).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Similarly, get the dev feature matrix with associated dev targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_x = \n",
    "dev_y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle method for training is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p],b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the Linear Regression class that inherits from the Pytorch nn module. Information about __init__ and __forward__ can be found in the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.final_layer = nn.Linear(num_features, 1)\n",
    "    def forward(self, x):\n",
    "        return self.final_layer(x)\n",
    "linreg_model = LinearRegression(num_features)\n",
    "\n",
    "optimizer = optim.SGD(linreg_model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### TODO: replace the SGD optimizer by using the Adam optimizer with learning rate 0.001. Check the Pytorch documentation for the optim package for more information: <http://pytorch.org/docs/master/optim.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Mean Squared Error as loss criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __np_to_var__ method is a convinience method used later to convert numpy arrays to Pytorch Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_var(np_array):\n",
    "    return Variable(torch.from_numpy(np_array).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training iterates over the data 100 times. The gradient is backpropagated after every training example and total loss is printed at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss_accum = 0.0\n",
    "    #train_x, train_y = unison_shuffled(train_x, train_y)\n",
    "    for i in range(len(train_y)):\n",
    "        x_i = np_to_var(train_x[i])\n",
    "        y_i = np_to_var(train_y[i])\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = linreg_model.forward(x_i)\n",
    "        loss = criterion(output, y_i)\n",
    "        loss_accum += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    print(\"train loss:\", loss_accum/len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: What effect can you observe by shuffling the data in each epoch? (Hint: Uncomment the unison_shuffled method) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### TODO: Extend the code that it additionally prints the loss on development data at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: \n",
    "Now implement the regression with a hidden layer and a activation function after that hidden layer. \n",
    "\n",
    "Recall, that normal linear regression uses the function:\n",
    "\n",
    "$$\\hat y = Linear(x) = Wx+b$$\n",
    "\n",
    "Neural Network regression, uses two (distinct) linear transformations, and a so-called ReLU (rectified linear unit) in between. Here we have:\n",
    "\n",
    "$$\\hat y = Linear(ReLU(Linear(x))) = W_B max(0, W_Ax+b_A) + b_B$$\n",
    "\n",
    "The output of the first linear transformation (+ReLU) is also called hidden layer.\n",
    "You can specify its size (hidden_size) for example to 10.\n",
    "\n",
    "\n",
    "You need to change the __init__ and the __forward__ method.\n",
    "(The ReLU activation is pre-defined, check the slides/PyTorch documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkRegression(nn.Module):\n",
    "    def __init__(self, num_features,hidden_size):\n",
    "        super(NeuralNetworkRegression, self).__init__()\n",
    "        \n",
    "        self.final_layer = nn.Linear(num_features, 1)\n",
    "    def forward(self, x):\n",
    "        return self.final_layer(x)\n",
    "nnreg_model = NeuralNetworkRegression(num_features,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: How does the Neural Network Regression compare to Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "#### Derive the gradient of the negative log likelihood for logistic regression (3 points)\n",
    "\n",
    "Negative log likelihood for logistic regression:\n",
    "\n",
    "$$ NLL(\\vec \\theta) =  - \\sum_{i=1}^m y^{(i)} \\log \\sigma(\\vec{\\theta}^T \\vec{x^{(i)}}) + (1-y^{(i)}) \\log (1 - \\sigma(\\vec{\\theta}^T \\vec{x^{(i)}}))$$\n",
    "\n",
    "Derive the expression for\n",
    "\n",
    "$$ \\nabla_\\theta NLL(\\vec \\theta) $$\n",
    "\n",
    "For each step give the exact rule you used.\n",
    "\n",
    "Note that you can use latex math syntax in notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
