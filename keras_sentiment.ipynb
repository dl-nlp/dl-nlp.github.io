{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP WS18/19\n",
    "## Exercise Sheet 7 - Keras Sentiment Prediction\n",
    "This exercise sheet is due on 11.12.18 11:59 pm. There is a total of **10 points**.\n",
    "Since some students wanted to try the functional API, there is an optional dot-product attention model to implement for additional bonus points. **You can get the full 10 points without the attention model.**\n",
    "\n",
    "Please work with Python 3 and in teams of 2 or 3 students, rename the file to keras_sentiment_last_names.ipynb and send your completed version to [anne.beyer@campus.lmu.de](mailto:anne.beyer@campus.lmu.de).\n",
    "\n",
    "As usual, you will have to complete the code/questions marked with ***TODO***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will predict the sentiment polarity (positive or negative) of IMDB movie reviews. \n",
    "You will implement different architectures in keras and comment on their performance relative to one another.\n",
    "\n",
    "Some of these models take a long time to train. For development, you can reduce the MAXEPOCHS parameter. Remember to restore the original value before evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plot\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 500\n",
    "BATCHSIZE = 16\n",
    "EMBSIZE = 50\n",
    "HIDDENSIZE = 50\n",
    "KERNELSIZE = 5\n",
    "VOCABSIZE = 10000\n",
    "\n",
    "MAXEPOCHS = 20\n",
    "#MAXEPOCHS = 2 # uncomment during development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path = \"imdb.npz\",\n",
    "                                                      num_words = VOCABSIZE,\n",
    "                                                      skip_top = 0,\n",
    "                                                      maxlen = MAXLEN,\n",
    "                                                      start_char = 1,\n",
    "                                                      oov_char = 2,\n",
    "                                                      index_from = 3)\n",
    "\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "y_test = np.expand_dims(y_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the validation set size to reduce runtime. **You should not do this in a serious experiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "print(\"# training samples: {}; # validation samples: {}\".format(len(x_train), len(x_test)))\n",
    "\n",
    "BATCHES_PER_EPOCH = len(x_train) // BATCHSIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use pad_sequences on x_train, x_test, the resulting matrices would become very big (num samples x size of longest sentence). Instead, we will use a generator function to pad the training data on-the-fly.\n",
    "\n",
    "**TODO: The following generator loops through x and y and yields tuples (x_batch, y_batch). x_batch and y_batch are slices of length BATCHSIZE from x and y. Complete the code in the places marked TODO (1 pt.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x, y, return_positions = False):\n",
    "    while True:\n",
    "        for i in range(0, len(x), BATCHSIZE):\n",
    "            x_batch = None # TODO\n",
    "            y_batch = None # TODO\n",
    "                \n",
    "            yield(pad_sequences(x_batch), y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic dropout layer for use below\n",
    "dropout_L = Dropout(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implement an bidirectional GRU model. You can use it as a template for the other models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model():\n",
    "    embedding_L = Embedding(input_dim = VOCABSIZE, output_dim = EMBSIZE, mask_zero = True)\n",
    "    gru_L = Bidirectional(GRU(units = HIDDENSIZE // 2))\n",
    "    output_L = Dense(units = 1, activation = \"sigmoid\")\n",
    "    return Sequential([embedding_L, dropout_L, gru_L, dropout_L, output_L])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Implement a 1-Dimensional CNN model. It should have the following layers: 1) Word embeddings 2) Dropout 3) 1-D convolution with kernel length KERNELSIZE and #filters HIDDENSIZE 4) global maximum pooling 5) Dropout 6) a linear layer with sigmoid activation (2.5 p.)**\n",
    "\n",
    "*Hint: CNNs don't accept input masks, so set mask_zero to False in the embedding layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():\n",
    "    return None # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On many tasks, a simple model that averages together word embeddings achieves surprisingly good performance. Therefore, it is a good idea to compare against this baseline.\n",
    "\n",
    "**TODO: Implement an embedding-only baseline. It should have the following layers: 1) Word embeddings 2) Dropout 3) global average pooling 4) Dropout 5) a linear layer with sigmoid activation (2.5 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emb_model():\n",
    "    return None # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two steps are **optional**, but note that there are **two more obligatory tasks below**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL:** Complete the following dot product attention layer. (2 p.) You can get full points without this exercise.\n",
    "\n",
    "*Hint: Attention gets a list of three tensors \\[query, key, value\\]. query has size (batch_size, 20), key has size (batch_size, T, 20), value has size (batch_size, T, HIDDENSIZE). Use backend functions such as K.batch_dot, K.softmax, K.sum in the call() method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        query_shape, key_shape, value_shape = input_shape\n",
    "        output_shape = None # TODO (output shape should be tuple with two entries)\n",
    "        return output_shape\n",
    "    \n",
    "    def call(self, inputs, mask = None):\n",
    "        query, key, value = inputs\n",
    "        \n",
    "        energy = None # TODO\n",
    "        energy /= np.sqrt(K.int_shape(key)[-1]) # scale by square root of Dkey\n",
    "        attention = None # TODO\n",
    "        output = None # TODO\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL:** Complete the following GRU+attention model. (2 p.) You can get full points without this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_attn_model():\n",
    "    input_L = Input((None,))\n",
    "    \n",
    "    embedding_L = Embedding(input_dim = VOCABSIZE, output_dim = EMBSIZE, mask_zero = True)\n",
    "    \n",
    "    gru_L = Bidirectional(GRU(units = HIDDENSIZE // 2, return_sequences = True))\n",
    "    \n",
    "    attn_L = Attention()\n",
    "    output_L = Dense(units = 1, activation = \"sigmoid\") \n",
    "    \n",
    "    key_L = TimeDistributed(Dense(units = 20))\n",
    "    query_L = Dense(units = 20)\n",
    "    value_L = TimeDistributed(Dense(units = HIDDENSIZE))\n",
    "    \n",
    "    # We will calculate our query from the last GRU hidden vector h_T.\n",
    "    # However, the GRU returns the full matrix of hidden vectors H.\n",
    "    # We therefore need a layer that takes H and returns h_T.\n",
    "    # TODO: Implement last_vector and last_vector_shape\n",
    "    \n",
    "    def last_vector(x):\n",
    "        return None # TODO\n",
    "    \n",
    "    def last_vector_shape(shape):\n",
    "        return None # TODO\n",
    "    \n",
    "    last_vector_L = Lambda(last_vector, output_shape = last_vector_shape)\n",
    "    \n",
    "    H = gru_L(embedding_L(input_L))\n",
    "    h_t = last_vector_L(H)\n",
    "    \n",
    "    query = query_L(h_t)\n",
    "    keys = key_L(H)\n",
    "    values = value_L(H)\n",
    "    \n",
    "    c_t = None # TODO\n",
    "    \n",
    "    output = output_L(concatenate([h_t, c_t]))\n",
    "    \n",
    "    model = Model(inputs = [input_L], outputs = [output]) # TODO\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Complete the compile function with the appropriate loss function, an optimizer of your choice and accuracy as a metric (1 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.compile() # TODO\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor = \"val_acc\", patience = 7)\n",
    "    \n",
    "    history = model.fit_generator(generator(x_train, y_train),\n",
    "                                  steps_per_epoch = BATCHES_PER_EPOCH,\n",
    "                                  validation_data = (pad_sequences(x_test), y_test),\n",
    "                                  epochs = MAXEPOCHS, callbacks = [earlystop])\n",
    "    \n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate, uncomment all models that you have implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "#uncomment all models that you have implemented:\n",
    "#models[\"gru+attn\"] = build_gru_attn_model()\n",
    "#models[\"gru\"] = build_gru_model()\n",
    "#models[\"cnn\"] = build_cnn_model()\n",
    "#models[\"emb\"] = build_emb_model()\n",
    "\n",
    "histories = {}\n",
    "traintimes = {}\n",
    "\n",
    "for name in sorted(models.keys()):\n",
    "    print(\"Training\", name)\n",
    "    before = time.time()\n",
    "    histories[name] = train_model(models[name], x_train, y_train, x_test, y_test)\n",
    "    duration = time.time() - before\n",
    "    traintimes[name] = duration / len(histories[name][\"loss\"]) / BATCHES_PER_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have measured the training time for every architecture, we can compare their efficiency. **Important:** This comparison is only meaningful with respect to your current hardware. For instance, parallelization is more effective on GPUs than on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average training time per batch\")\n",
    "for name in traintimes:\n",
    "    print(name, round(traintimes[name], 4), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot training and validation set metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, metric in enumerate(sorted(histories[\"gru\"].keys())):\n",
    "    plot.subplot(1, len(histories[\"gru\"]), i+1)\n",
    "    \n",
    "    for name in sorted(histories.keys()):\n",
    "        plot.plot(range(1, len(histories[name][metric]) + 1),\n",
    "                        histories[name][metric], label = name)\n",
    "        plot.title(metric)\n",
    "        plot.xlabel(\"Epoch\")\n",
    "    \n",
    "plot.tight_layout()\n",
    "plot.legend()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Compare the different architectures in a few sentences. You should comment on overall performance, overfitting and convergence time (3 p.)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
