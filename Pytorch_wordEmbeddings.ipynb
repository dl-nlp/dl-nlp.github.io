{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP WS17/18\n",
    "## Exercise Sheet 4 - Pytorch WordEmbeddings\n",
    "This exercise sheet is due on 28.11.17 11:59 pm. There is a total of 5\n",
    "points for this exercise sheet. Please send your solution in a\n",
    "suitable format to [beroth@cis.uni-muenchen.de](mailto:beroth@cis.uni-muenchen.de). Please submit a\n",
    "completed version of this file in Python 3. You may submit in teams of\n",
    "2 or 3 students.\n",
    "\n",
    "\n",
    "You will have to complete the code where marked with ***TODO***\n",
    "\n",
    "Please rename the file to pytorch_wordEmbeddings_last_names.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please refer to the last exercise sheet if you have trouble installing Pytorch. This time you will have to install nltk. (e.g. by using pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk library provides datasets which can be used for dry running an approach or to verify a hypothesis. We will use the brown corpus. In case you never used the brown corpus before on your machine you will have to uncomment the download statement.\n",
    "\n",
    "For developing/debugging your code you can only take the first tokens of the corpus. Training with the full corpus may take up to one hour (standard laptop). Use only the first 1000 words until your code works, later you can use the whole corpus. Note that the words used for human evaluation at the very end could not be part of the short list!\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"brown\")\n",
    "#brown_word_list = [w.lower() for w in brown.words()] #TODO\n",
    "brown_word_list = [w.lower() for w in brown.words()[:1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the vocabulary. The brown corpus contains 56057 unique words/tokens (lower cased), for the sake of computation time we will only use the 10 000 most common words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary\n",
    "word_counts = Counter(brown_word_list)\n",
    "vocab_size = 10000  # \n",
    "vocab = {w[0]: idx for idx, w in enumerate(word_counts.most_common(vocab_size))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some Hyper-parameters. In case it is not clear what they do read it up or ask your Tutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of the algorithm\n",
    "window_size = 1 # window size\n",
    "# TODO: You can change the number of negative samples to a higher value, e.g. 7. This will give you better results, but will take longer to train.\n",
    "neg_samples_factor = 3 # negative samples multiple\n",
    "dims = 64 # embedding dimension\n",
    "learning_rate = 0.01\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a list of strings (words) and returns a generator of word tuples of the following form:\n",
    "    \n",
    "(center_word, context_word, True)\n",
    "\n",
    "where center_word is the word at a certain position, and context_word is at most max_distance tokens away.\n",
    "\n",
    "\"True\" is the label of that pair (tuples in positive_cooccurrences always have the label \"True\").\n",
    "\n",
    "Words are represented by integers (rather than by string) denoting their id.\n",
    "\n",
    "Only pairs where both words are in the vocabulary are considered.\n",
    "\n",
    "Note: cooccurrence only holds between words in different positions, not for a position with itself.)\n",
    "\n",
    "    :param tokens: list of strings (words)\n",
    "    :param max_distance: max distance of context word to target word\n",
    "    :param neg_samples_factor: number of sampled negative tuples for each positive tuple\n",
    "    :param vocab_to_id: dictionary (string to int) mapping each word to its id (=row in embedding matrizes).\n",
    "    :return: generator over tuples of the form (context_word:string, center_word:string, label:boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_and_negative_cooccurrences(tokens, max_distance, neg_samples_factor, vocab_to_id):\n",
    "    for center_position in range(len(tokens)):\n",
    "        center_word = tokens[center_position]\n",
    "        if center_word not in vocab_to_id:\n",
    "            continue\n",
    "        context_start = max(0, center_position - max_distance)\n",
    "        context_end = min(len(tokens), center_position + max_distance + 1)\n",
    "        for context_position in range(context_start, context_end):\n",
    "            if context_position != center_position:\n",
    "                context_word = tokens[context_position]\n",
    "                if context_word not in vocab_to_id:\n",
    "                    continue\n",
    "                yield (vocab_to_id[center_word], vocab_to_id[context_word], True)\n",
    "                for i in range(neg_samples_factor):\n",
    "                    yield (vocab_to_id[center_word], random.randint(0, len(vocab_to_id) - 1), False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build our model. As you have seen in the last exercise you define a model by making a class inherit from torch.nn.module as well as defining the init and forward method. \n",
    "\n",
    "##### init method\n",
    "\n",
    "***TODO*** You will have to create the embedddings for the vocabulary as context and target/center words. Hint: There is a Embedding layer in Pytorch which should be used here. (1p)\n",
    "\n",
    "#### forward method:\n",
    "\n",
    "***TODO*** What is the shape of center_context_idxs? (1p)\n",
    "\n",
    "***TODO*** What are the shapes of cntr_idxs and ctxt_idxs? (1p)\n",
    "\n",
    "***TODO***  ctxt_vecs must have the correct shape for batch-wise matrix multiplication (use variable.view(...) ) Check the documentation for torch.bmm(...) to see what is the required shape. (1p)\n",
    "\n",
    "***TODO*** What is the shape of the returned Variable? (1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        # TODO: create embeddings for vocabulary as context and target words (1p)\n",
    "        self.embeddings_center = None # TODO\n",
    "        self.embeddings_context = None #TODO\n",
    "\n",
    "    def forward(self, center_context_idxs):\n",
    "        # TODO: What is the shape of center_context_idxs? (1p)\n",
    "        cntr_idxs = center_context_idxs[:, 0]\n",
    "        ctxt_idxs = center_context_idxs[:, 1]\n",
    "        # TODO: What are the shapes of cntr_idxs and ctxt_idxs? (1p)\n",
    "\n",
    "        cntr_vecs = self.embeddings_center(ctxt_idxs).view(-1, 1, self.embedding_size) # resulting shape: batch_size x 1 x embedding_size\n",
    "       \n",
    "        ctxt_vecs = self.embeddings_context(cntr_idxs) #TODO\n",
    "        # TODO: ctxt_vecs must have the correct shape for batch-wise matrix multiplication (use variable.view(...) ) ...\n",
    "        # TODO: ...Check the documentation for torch.bmm(...) to see what is the required shape. (1p)\n",
    "       \n",
    "        scores = torch.bmm(cntr_vecs, ctxt_vecs) # Batch-wise matrix multiplication. Resulting shape: batch_size x 1 x 1\n",
    "        return scores.view(-1,1) # TODO: What is the shape of the returned Variable? (1p)\n",
    "\n",
    "    def center_sims(self, word_idx):\n",
    "        m = self.embeddings_center.weight\n",
    "        v = m[word_idx]\n",
    "        return F.cosine_similarity(m, v.expand(m.size())).data.numpy()\n",
    "\n",
    "    def context_sims(self, word_idx):\n",
    "        m = self.embeddings_context.weight\n",
    "        v = m[word_idx]\n",
    "        return F.cosine_similarity(m, v.expand(m.size())).data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on there is nothing left to do for you. Please read through the following and have a look into the documentation to see what each line does. \n",
    "\n",
    "Understanding how the model learns and predicts will be essential for your upcoming projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(vocab_size, dims)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(w2v_model.parameters(), lr=learning_rate)\n",
    "\n",
    "pos_neg_list = list(positive_and_negative_cooccurrences(brown_word_list, window_size, neg_samples_factor, vocab))\n",
    "data_size=len(pos_neg_list)\n",
    "train_data = np.asarray(pos_neg_list)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "data_tensor = torch.LongTensor(train_data[:, 0:2].tolist())\n",
    "target_tensor = torch.FloatTensor(train_data[:, 2].tolist()).view(-1,1)\n",
    "\n",
    "train = data_utils.TensorDataset(data_tensor, target_tensor)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch_nr in range(num_epochs):\n",
    "    loss_accum = 0.0\n",
    "    print(\"epoch\", epoch_nr)\n",
    "    for ctxt_tgt_idxs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = w2v_model.forward(Variable(ctxt_tgt_idxs))\n",
    "        loss = criterion(output, Variable(labels))\n",
    "        loss_accum += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"current loss:\", loss_accum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we get the most similar word embeddings to a few example words. The first list holds the results from the word being the center word, the second from the word being a context word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_words = sorted(vocab.keys(), key=vocab.get)\n",
    "def top_words_for(word, n=10, for_center=True):\n",
    "    print(word)\n",
    "    query_index = vocab[word]\n",
    "    if for_center:\n",
    "        sims = list(zip(w2v_model.center_sims(query_index), sorted_words))\n",
    "    else:\n",
    "        sims = list(zip(w2v_model.context_sims(query_index), sorted_words))\n",
    "    sims.sort(key=lambda x: -x[0])\n",
    "    return sims[:n]\n",
    "\n",
    "print(\"word\".ljust(20), \"similarity\")\n",
    "for ww in [\"before\", \"president\", \"city\", \"man\", \"government\",\"1\",\"two\"]:\n",
    "    print(\"=\"*35)\n",
    "    for dist, nw in top_words_for(ww, for_center=True):\n",
    "        print(nw.ljust(20), dist)\n",
    "    for dist, nw in top_words_for(ww, for_center=False):\n",
    "        print(nw.ljust(20), dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
