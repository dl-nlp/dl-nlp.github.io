{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using LSTM and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 10 points in total for this homework. Send the completed notebook to beroth@cis.uni-muenchen.de. The deadline is Tuesday, December 19, 23:59. You can work in teams of 2 or 3. This is the last exercise before the projects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some imports. You will have to add imports for the CNN in the second part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from keras.layers import add as addition\n",
    "import sys\n",
    "#TODO\n",
    "random.seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part helps loading the sentiment data into the needed format. We will use the movie review corpus, which is provided by the nltk package. It classifies movie reviews as positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_data(n_texts_train=1500, n_texts_dev=500, vocab_size=10000):\n",
    "    \"\"\"\n",
    "    Reads texts from the nltk movie_reviews corpus. A word2id dictionary is \n",
    "    created and the words in the texts are substituted with their numbers. Training\n",
    "    and Development data is returned, together with labels and the word2id dictionary.\n",
    " \n",
    "    :param n_texts_train: the number of reviews that will form the training data\n",
    "    :param n_texts_dev: the number of reviews that will form the development data\n",
    "    :param vocab_size: the maximum size of the vocabulary.\n",
    "\n",
    "    :return list texts_train: A list containing lists of wordids corresponding to \n",
    "    training texts.\n",
    "    :return list texts_dev: A list containing lists of wordids corresponding to \n",
    "    development texts.\n",
    "    :return labels_train: A list containing the labels (0 or 1) for the corresponding\n",
    "    text entry in texts_train\n",
    "    :return labels_dev: A ilst containing the labels (0 or 1) for the corresponding\n",
    "    text entry in texts_dev\n",
    "    :return word2id: The dictionary obtained from the training texts that maps each\n",
    "    seen word to an id.\n",
    "    \"\"\"\n",
    "    all_ids = movie_reviews.fileids()\n",
    "    if (n_texts_train+n_texts_dev>len(all_ids)):\n",
    "        print (\"Error: There are only\",len(all_ids), \"texts in the movie_reviews corpus. Training with all of those sentences.\")\n",
    "        n_texts_train=1500\n",
    "        n_texts_dev=500\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "    random.shuffle(all_ids)\n",
    "\n",
    "    texts_train=[]\n",
    "    labels_train=[]\n",
    "    texts_dev=[]\n",
    "    labels_dev=[]\n",
    "\n",
    "    for i in range(n_texts_train):\n",
    "        text = movie_reviews.raw(fileids=[all_ids[i]])\n",
    "        tokens = [word.lower() for word in word_tokenize(text)]\n",
    "        texts_train.append(tokens)\n",
    "        if all_ids[i] in posids:       \n",
    "            labels_train.append(1)\n",
    "        else:\n",
    "            labels_train.append(0)\n",
    "\n",
    "    for i in range(n_texts_train, n_texts_train+n_texts_dev):\n",
    "        text = movie_reviews.raw(fileids=[all_ids[i]])\n",
    "        tokens = [word.lower() for word in word_tokenize(text)]\n",
    "        texts_dev.append(tokens)\n",
    "        if all_ids[i] in posids:\n",
    "            labels_dev.append(1)\n",
    "        else:\n",
    "            labels_dev.append(0)\n",
    "\n",
    "    word2id=create_dictionary(texts_train, vocab_size)\n",
    "    texts_train = [to_ids(s,word2id) for s in texts_train]\n",
    "    texts_dev = [to_ids(s,word2id) for s in texts_dev]\n",
    "    return (texts_train, labels_train, texts_dev, labels_dev, word2id)\n",
    "\n",
    "def create_dictionary(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a dictionary that maps words to ids. More frequent words have lower ids.\n",
    "    The dictionary contains at the vocab_size-1 most frequent words (and a placeholder '<unk>' for unknown words).\n",
    "    The place holder has the id 0.\n",
    "    \"\"\"\n",
    "    counter = collections.Counter()\n",
    "    for tokens in texts:\n",
    "        counter.update(tokens)\n",
    "    vocab = [w for w,c in counter.most_common(vocab_size-1)]\n",
    "    word_to_id = {w:(i+1) for i,w in enumerate(vocab)} \n",
    "    word_to_id[UNKNOWN_TOKEN] = 0 \n",
    "    return word_to_id \n",
    "\n",
    "def to_ids(words, dictionary):\n",
    "    \"\"\"\n",
    "    Takes a list of words and converts them to ids using the word2id dictionary.\n",
    "    \"\"\"\n",
    "    ids=[]\n",
    "    for word in words:\n",
    "        ids.append(dictionary.get(word, dictionary[UNKNOWN_TOKEN]))\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a couple of constants, which should be familiar from the last exercise and fetch the data. You don't have to remove the download part, it will check automatically if it is already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 20\n",
    "HIDDEN_SIZE = 10\n",
    "EPOCHS = 10\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "x_train, y_train, x_dev, y_dev, word2id = nltk_data(vocab_size=VOCAB_SIZE)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_dev = sequence.pad_sequences(x_dev, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first of three models that we will define is a Bidirectional LSTM. In comparison to the last exercise when you had to get an output at each timestep, this time you will only need the last output.\n",
    "\n",
    "** TODO: Build model with bidirectional LSTM, with HIDDEN_SIZE size for each direction. After the LSTM, insert one additional dense layer (HIDDEN_SIZE and tanh non-linearity), before the label is predicted by the final layer.(1 p.) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "lstm_model.add() # TODO\n",
    "lstm_model.add() # TODO\n",
    "lstm_model.add() # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_dev, y_dev))\n",
    "score, acc = lstm_model.evaluate(x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM Accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: Solve the same problem using a CNN+pooling over three-grams with 2\\*HIDDEN_SIZE filters, and apply the tanh non-linearity. After CNN+pooling, insert as before one additional dense layer (HIDDEN_SIZE and tanh non-linearity), before the label is predicted by the final layer. (3 p.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential() \n",
    "cnn_model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "cnn_model.add() # TODO\n",
    "cnn_model.add() # TODO\n",
    "cnn_model.add() # TODO\n",
    "cnn_model.add() # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_dev, y_dev))\n",
    "score, acc = cnn_model.evaluate(x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: How many parameters in total does the entire CNN model (all layers including embedding) have to optimize/learn? Show your calculation. (1.5 p.) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: Use the Keras functional API to merge (combine) the LSTM sentence representation with the pooled representation. Instead of concatenation use vector addition for merging. The same embedding should be learned and used going into the LSTM and the CNN. As before, insert one additional dense layer (HIDDEN_SIZE and tanh non-linearity), before the label is predicted by the final layer. Train and evaluate as before. (5.5 p.)**\n",
    "functional API: https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compostion Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
