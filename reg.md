## Regularization 

| Technique | What? | How? | Links |
|-----------------------------|:--------------------------------:|:------:|:-------------------------------------------------------------------|
| L1 | Minimize weight values | ```from keras import regularizers``` | [http](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)|
| L2 | Minimize weight values. Bigger Values are heavier penalized compared to L1| ```from keras import regularizers``` |[http](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/) |
| Dropout | Drop random neurons from a layer. Information needs to be encoded in multiple pathways. | ```from keras.layers import Dropout``` | [http](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)|
| Early-Stopping | |  | |
| Limit layer size | |  | |
| Data augmentation | |  | |
